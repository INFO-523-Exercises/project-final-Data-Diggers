---
title: "Seeing the Unseen"
subtitle: "Predicting Brain Stroke in a Patient"
author: "Data Diggers"
format: html
editor: visual
---

## Abstract

Strokes are a significant public health concern worldwide, and predicting the risk factors associated with strokes can contribute to early intervention and improved patient outcomes. This project aims to implement machine learning models on a data pretaning to stroke occurrences.

```{r, echo=FALSE}
library(here)

brain_stroke_data = read.csv(here("data","brain_stroke.csv"))
```

## Introduction

The emergence of advanced healthcare datasets has paved the way for groundbreaking research and development in the realm of medical science. In our pursuit of leveraging data-driven insights to address critical health issues, we have undertaken a comprehensive analysis of the Brain Stroke Dataset sourced from Kaggle. This dataset, offers a rich repository of patient records and associated attributes, presenting a unique opportunity for in-depth examination and exploration.

The primary impetus behind this project is the compelling need to revolutionize early stroke detection and refine risk assessment methodologies. Brain strokes, with their potentially devastating consequences, underscore the significance of timely intervention. By harnessing the power of data analytics, we aim to unravel patterns and correlations within the dataset that can contribute to the development of predictive models. These models, in turn, hold the potential to augment healthcare professionals' ability to identify individuals at risk of strokes at an early stage.

Our motivation stems from the realization that proactive measures in stroke prediction can significantly enhance patient care and facilitate more effective stroke prevention strategies. By delving into the intricacies of patient records and extracting meaningful insights, we aspire to contribute to the growing body of knowledge that aids medical practitioners in making informed decisions.

## Question 1: How accurate are various classification models for detecting a stroke in a patient?

## Approach

-   In the initial phase of our brain stroke prediction project, we conducted comprehensive examination of the dataset. The **brain.shape** and **brain.info()** checks ensured the dataset's integrity, revealing its dimensions and basic information. Notably, no missing values were found (**brain.isnull().sum()**).Â 

-   Visualizations were created to delve into correlations, including work types and smoking statuses, and the likelihood of heart disease based on gender, age, and work type.

-   We develoepd classification models using Logistic Regression, K-Nearest Neighbors, Random Forest and Decision Tree and picked the best model in based on accuracy value.

```{r, echo=FALSE}
if (!require(pacman))
  install.packages("pacman")

pacman::p_load(tidymodels,
               tidyverse,
               ranger,
               gridExtra,
               dplyr,
               corrplot,
               MASS)


```

## Analysis and Visualizations

![](images/Heatmap.png)

Heatmap colors show correlation strength: green shades mean strong, red shades mean weak. Cell numbers are correlation coefficients (0 to 1). Positive Correlation (close to 1) means variables increase together. No Correlation (close to 0) means little or no linear relationship.

![](images/stroke_smoking.png){fig-align="center" width="3288"}

Left plot Interpretation: This subplot helps visualize the distribution of ages for different smoking statuses, split by whether individuals had a stroke or not.

Right plot Interpretation: This subplot visualizes the distribution of ages for different genders, split by whether individuals had a stroke or not.

```{r, echo=FALSE}
#| label: init data
brain <- read.csv("data/brain_stroke.csv")
```

```{r, echo=FALSE}
#| label: packages
if (!require(pacman))
  install.packages("pacman")

pacman::p_load(flexdashboard,
              ggplot2,
               corrplot,
              dplyr)
```

```{r, echo=FALSE}
#| label: modify data for use
# our data need to be numeric so lets temporarily remove some columns that we are not that curious about 
# lets change gender to 1 being male and 0 being female. 
# We will also change smoking to be 0 for never smoking and 1 to smoking, unknown smoking will be counted as not smoking
numeric_brain <- brain %>% dplyr::select(-Residence_type, -work_type, -ever_married) %>%
  mutate(gender = if_else(gender == "Male", 1, 0)) %>%
  mutate(smoking_status = if_else(smoking_status == "never smoked", 0, 1))

```

```{r, echo=FALSE}
#| label: scatter plot age, glucose and stroke
# create scatter plot with 3 variable 1 being a factor and 2 being numeric
ggplot(brain, aes(x = age, y = avg_glucose_level, color = factor(stroke))) +
  geom_point(aes(alpha = factor(stroke)), size = 2) +
  scale_color_manual(values = c("lightblue", "red"), name = "stroke", labels = c("no", "yes")) +
  scale_alpha_manual(values = c(.2, 1)) +
  labs(title = "Age vs. Average Glucose Level") +
  ylab("Average Glucose Level") +
  xlab("Age") +
  guides(alpha = "none")
```

A combination of age and glucose levels plays a big factor into having a stroke or not. As we notice a dense population of strokes in high age plus high glucose levels.

```{r, echo = FALSE}
#| label: bar plot with diesease and strokes
# create stacked bar count percentage plot with 3 variable being factors
ggplot(brain, aes(x = hypertension, fill = as.factor(stroke))) +
  geom_bar(position = "fill", color = "black") +
  facet_grid(. ~ heart_disease, labeller = labeller(heart_disease = c("0" = "Without Heart Disease", "1" = "With Heart Disease"))) +
  labs(title = "Distribution of Stroke Status by hypertension and heart disease",
       x = "hypertension", y = "Percentage Count", fill = "Stroke") +
  scale_fill_manual(values = c("lightblue",  "red"))

```

There is definitely noticeable correlation here as having more preexisting conditions will increase the chance at having a stroke, while having none conditions significantly lowers the chance.

## Mining Method

In this project, we have used the following four classification models -

**Logistic Regression**

Despite its name, Logistic Regression is a binary classification algorithm. It models the probability of a binary outcome using a logistic function. In stroke prediction, logistic regression is employed to estimate the likelihood of stroke occurrence based on given features, contributing to the understanding of the relationship between various factors and stroke risk.

**K-Nearest Neighbors (KNN)**

KNN is a classification algorithm that classifies a data point by considering the class labels of its K-nearest neighbors in the feature space. In stroke prediction, KNN is employed to identify patterns by assessing the similarity of instances, contributing to the assessment of stroke risk based on feature patterns.

**Naive Bayes**

Naive Bayes is a classification algorithm based on Bayes' theorem, assuming independence between features. In stroke prediction, it models the probability of stroke occurrence based on given features, assuming a Gaussian distribution for continuous features and feature independence.

**Decision Tree**

Decision trees are classification algorithms that recursively split the dataset into subsets based on significant features. They form a tree-like structure where each branch represents a decision based on specific features. In stroke prediction, decision trees are valuable for identifying key features and their interactions that contribute to predicting the likelihood of strokes.

**Random Forest**

Random Forests, an ensemble method, consist of multiple decision trees. They construct these trees during training and output the mode of classes or mean prediction of individual trees. In stroke prediction, Random Forests are employed to capture complex relationships within the dataset by aggregating predictions from multiple decision trees, providing a robust approach to assess stroke risk.

## Code

```{r}


```

## Question 2: How are certain lifestyle changes ranked on the basis of their importance in reducing the possibility of a stroke?

## Approach:

-   The feature ranking provided by the logistic regression model with their corresponding coefficients gives insights into the impact of each feature on the likelihood of the target variable. In logistic regression, the coefficients represent the change in the log-odds of the target variable for a one-unit change in the corresponding feature.

-   The feature ranking provided by the logistic regression model with their corresponding coefficients gives insights into the impact of each feature on the likelihood of the target variable. In logistic regression, the coefficients represent the change in the log-odds of the target variable for a one-unit change in the corresponding feature.

## Discussion:

### Here's an interpretation of the feature ranking :

-   **age_band (Coefficient: 1.187519):**

    For every one-unit increase in the age_band, the log-odds of the target variable increase by approximately 1.187519. Older age is associated with a higher likelihood of the target variable.

-   **avg_glucose_level (Coefficient: 0.880860):**

    For every one-unit increase in avg_glucose_level, the log-odds of the target variable increase by approximately 0.880860. Higher average glucose levels are associated with a higher likelihood of the target variable.

-   **hypertension (Coefficient: 0.551390):**

    Individuals with hypertension have a log-odds of the target variable approximately 0.551390 higher than those without hypertension. Hypertension is associated with a higher likelihood of the target variable.

-   **heart_disease (Coefficient: 0.288187):**

    Individuals with heart disease have a log-odds of the target variable approximately 0.288187 higher than those without heart disease. Heart disease is associated with a higher likelihood of the target variable.

-   **gender (Coefficient: 0.073007):**

    Being of the male gender is associated with a log-odds of the target variable approximately 0.073007 higher than being of the female gender. Gender has a positive but relatively small association with the likelihood of the target variable.

-   **smoking_status (Coefficient: -0.053451):**

    For every one-unit increase in smoking_status, the log-odds of the target variable decrease by approximately 0.053451. Smoking is associated with a lower likelihood of the target variable.

-   **bmi (Coefficient: -0.211967):**

    For every one-unit increase in BMI, the log-odds of the target variable decrease by approximately 0.211967. Higher BMI is associated with a lower likelihood of the target variable.

\
